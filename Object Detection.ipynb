{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset disponible sur https://www.kaggle.com/datasets/huanghanchina/pascal-voc-2012?resource=download\n",
    "\n",
    "Pour tester quelques images (attention c'est un gros dataset -W 27Go): https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from tensorflow.keras.layers import (Conv2D, Dense, Flatten, BatchNormalization, Dropout, Reshape, LeakyReLU)\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "train_images = 'VOC2012/JPEGImages/'\n",
    "train_maps = 'VOC2012/Annotations/'\n",
    "\n",
    "val_images = 'VOC2012/ValJPEGImages/'\n",
    "val_maps = 'VOC2012/ValAnnotations/'\n",
    "\n",
    "classes = ['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable', 'dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']\n",
    "\n",
    "B = 2\n",
    "N_CLASSES = len(classes)\n",
    "H,W = 224,224\n",
    "SPLIT_SIZE = H // 32\n",
    "print(SPLIT_SIZE)\n",
    "N_EPOCHS = 135\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list = ['2007_000027.jpg','2007_000032.jpg','2007_000033.jpg','2007_000039.jpg','2007_000042.jpg','2007_000061.jpg',\n",
    "            '2007_000063.jpg','2007_000068.jpg','2007_000121.jpg','2007_000123.jpg','2007_000129.jpg','2007_000170.jpg',\n",
    "            '2007_000175.jpg','2007_000187.jpg','2007_000241.jpg','2007_000243.jpg','2007_000250.jpg','2007_000256.jpg',\n",
    "            '2007_000272.jpg','2007_000323.jpg','2007_000332.jpg','2007_000333.jpg','2007_000346.jpg','2007_000363.jpg',\n",
    "            '2007_000364.jpg','2007_000392.jpg','2007_000423.jpg','2007_000452.jpg','2007_000464.jpg','2007_000480.jpg',\n",
    "            '2007_000491.jpg','2007_000504.jpg','2007_000515.jpg','2007_000528.jpg','2007_000529.jpg','2007_000549.jpg',\n",
    "            '2007_000559.jpg','2007_000572.jpg','2007_000584.jpg','2007_000629.jpg','2007_000636.jpg','2007_000645.jpg',\n",
    "            '2007_000648.jpg','2007_000661.jpg','2007_000663.jpg','2007_000664.jpg','2007_000676.jpg','2007_000713.jpg',\n",
    "            '2007_000720.jpg','2007_000727.jpg','2007_000733.jpg','2007_000738.jpg','2007_000762.jpg','2007_000768.jpg',\n",
    "            '2007_000783.jpg','2007_000793.jpg','2007_000799.jpg','2007_000804.jpg','2007_000807.jpg','2007_000822.jpg',\n",
    "            '2007_001299.jpg','2007_001311.jpg','2007_001321.jpg','2007_001340.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in val_list:\n",
    "#     shutil.move(train_maps+name[:-3]+\"xml\", val_maps+name[:-3]+\"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in val_list:\n",
    "#     shutil.move(train_images+name, val_images+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_xml(filename):\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    size_tree = root.find('size')\n",
    "    height = float(size_tree.find('height').text)\n",
    "    width = float(size_tree.find('width').text)\n",
    "    bounding_boxes = []\n",
    "    \n",
    "    for object_tree in root.findall('object'):\n",
    "        for bounding_box in object_tree.iter('bndbox'):\n",
    "            xmin = (float(bounding_box.find('xmin').text))\n",
    "            ymin = (float(bounding_box.find('ymin').text))\n",
    "            xmax = (float(bounding_box.find('xmax').text))\n",
    "            ymax = (float(bounding_box.find('ymax').text))\n",
    "            # print(xmin, ymin, xmax, ymax)\n",
    "            break\n",
    "        \n",
    "        class_name = object_tree.find('name').text\n",
    "        class_dict = {classes[i]:i for i in range(len(classes))}\n",
    "        bounding_box = [\n",
    "            (xmin+xmax)/(2*width), (ymin+ymax)/(2*height), (xmax-xmin)/width,\n",
    "            (ymax-ymin)/height, class_dict[class_name]]\n",
    "        bounding_boxes.append(bounding_box)\n",
    "    \n",
    "    return tf.convert_to_tensor(bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 5), dtype=float32, numpy=\n",
       "array([[ 0.479     ,  0.4644128 ,  0.542     ,  0.37366548,  0.        ],\n",
       "       [ 0.33      ,  0.37544483,  0.128     ,  0.12455516,  0.        ],\n",
       "       [ 0.408     ,  0.727758  ,  0.036     ,  0.17437722, 14.        ],\n",
       "       [ 0.07      ,  0.7597865 ,  0.036     ,  0.17437722, 14.        ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_xml(val_maps + \"2007_000032.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(bounding_boxes):\n",
    "    output_label=np.zeros((SPLIT_SIZE,SPLIT_SIZE,N_CLASSES+5))\n",
    "    for b in range(len(bounding_boxes)):\n",
    "        grid_x = bounding_boxes[..., b, 0] * SPLIT_SIZE\n",
    "        grid_y = bounding_boxes[..., b, 1] * SPLIT_SIZE\n",
    "        i = int(grid_x)\n",
    "        j = int(grid_y)\n",
    "\n",
    "        output_label[i, j, 0:5] = [1., grid_x%1, grid_y%1, bounding_boxes[...,b,2], bounding_boxes[...,b,3]]\n",
    "        output_label[i, j, 5+int(bounding_boxes[..., b, 4])] = 1.\n",
    "\n",
    "    return tf.convert_to_tensor(output_label,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25,), dtype=float32, numpy=\n",
       "array([1.        , 0.35299993, 0.25088978, 0.542     , 0.37366548,\n",
       "       1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_output(preprocess_xml(val_maps+\"2007_000032.xml\"))[3][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17061 17061\n",
      "64 64\n"
     ]
    }
   ],
   "source": [
    "im_paths = []\n",
    "xml_paths = []\n",
    "\n",
    "val_im_paths = []\n",
    "val_xml_paths = []\n",
    "\n",
    "for i in os.listdir(train_maps):\n",
    "    im_paths.append(train_images+i[:-3]+'jpg')\n",
    "    xml_paths.append(train_maps+i)\n",
    "\n",
    "for i in os.listdir(val_maps):\n",
    "    val_im_paths.append(val_images+i[:-3]+'jpg')\n",
    "    val_xml_paths.append(val_maps+i)\n",
    "\n",
    "print(len(im_paths), len(xml_paths))\n",
    "print(len(val_im_paths), len(val_xml_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((im_paths, xml_paths))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_im_paths, val_xml_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'VOC2012/ValJPEGImages/2007_000027.jpg'>, <tf.Tensor: shape=(), dtype=string, numpy=b'VOC2012/ValAnnotations/2007_000027.xml'>)\n"
     ]
    }
   ],
   "source": [
    "for i in val_dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imbboxes(im_path, xml_path):\n",
    "    img = tf.io.decode_jpeg(tf.io.read_file(im_path))\n",
    "    img = tf.cast(tf.image.resize(img, size=[H, W]), dtype=tf.float32)\n",
    "\n",
    "    bboxes = tf.numpy_function(func=preprocess_xml, inp=[xml_path], Tout=tf.float32)\n",
    "    return img, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(get_imbboxes)\n",
    "val_dataset = val_dataset.map(get_imbboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3) tf.Tensor([[0.626      0.40727273 0.208      0.34181818 3.        ]], shape=(1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_dataset.skip(2):\n",
    "    print(i.shape, j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('out_1.jpg', np.array(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Resize(H, W),\n",
    "    A.RandomCrop(width=np.random.randint(int(0.9 * W), 224), height=np.random.randint(int(0.9 * H), 224), p=0.5),\n",
    "    A.RandomScale(scale_limit=0.1, interpolation=cv2.INTER_LANCZOS4, p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Resize(H, W)\n",
    "], bbox_params=A.BboxParams(format='yolo', min_area=25, min_visibility=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_albument(image, bboxes):\n",
    "    augmented = transforms(image=image, bboxes=bboxes)\n",
    "    return [tf.convert_to_tensor(augmented[\"image\"], dtype=tf.float32), tf.convert_to_tensor(augmented[\"bboxes\"], dtype=tf.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(image, bboxes):\n",
    "    aug = tf.numpy_function(func=aug_albument, inp=[image, bboxes], Tout=(tf.float32, tf.float32))\n",
    "    return aug[0], aug[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3) tf.Tensor([[0.62488073 0.43315092 0.21372478 0.3716858  3.        ]], shape=(1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_dataset.skip(2):\n",
    "    print(i.shape, j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('out_2.jpg', np.array(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_augment(img, y):\n",
    "    img = tf.image.random_brightness(img, max_delta=50.)\n",
    "    img = tf.image.random_saturation(img, lower=0.5, upper=1.5)\n",
    "    img = tf.image.random_contrast(img, lower=0.5, upper=1.5)\n",
    "    # img = tf.image.random_hue(img, max_delta=0.5)\n",
    "    img = tf.clip_by_value(img, 0, 255)\n",
    "    labels = tf.numpy_function(func=generate_output, inp=[y], Tout=(tf.float32))\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, bboxes):\n",
    "    img = tf.cast(tf.image.resize(img, size=[H, W]), dtype=tf.float32)\n",
    "\n",
    "    labels = tf.numpy_function(func=generate_output, inp=[bboxes], Tout=(tf.float32))\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(preprocess_augment)\n",
    "val_dataset = val_dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    train_dataset\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = (\n",
    "    val_dataset\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) tf.Tensor(\n",
      "[[[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [1.         0.79999983 0.11466646 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [1.         0.13699973 0.56266654 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [1.         0.37416512 0.6922506  ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [1.         0.7389999  0.0872485  ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]], shape=(32, 7, 7, 25), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_dataset.take(1):\n",
    "    print(i.shape, j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('out_3.jpg', np.array(i[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILTERS = 512\n",
    "OUTPUT_DIM = N_CLASSES + 5 * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = tf.keras.applications.resnet50.ResNet50(\n",
    "base_model = tf.keras.applications.efficientnet.EfficientNetB1(\n",
    "    weights = 'imagenet',\n",
    "    input_shape = (H, W, 3),\n",
    "    include_top = False\n",
    ")\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb1 (Functional)  (None, 7, 7, 1280)       6575239   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 7, 7, 512)         5898752   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 7, 7, 512)        2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 7, 7, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 7, 7, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 7, 7, 512)         2359808   \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               12845568  \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1470)              754110    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 30)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,161,285\n",
      "Trainable params: 26,581,950\n",
      "Non-trainable params: 6,579,335\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    Conv2D(NUM_FILTERS, (3, 3), padding = 'same', kernel_initializer = 'he_normal'),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "\n",
    "    Conv2D(NUM_FILTERS, (3, 3), padding = 'same', kernel_initializer = 'he_normal'),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "\n",
    "    Conv2D(NUM_FILTERS, (3, 3), padding = 'same', kernel_initializer = 'he_normal'),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "\n",
    "    Conv2D(NUM_FILTERS, (3, 3), padding = 'same', kernel_initializer = 'he_normal'),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(NUM_FILTERS, kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(SPLIT_SIZE * SPLIT_SIZE * OUTPUT_DIM, activation = 'sigmoid'),\n",
    "\n",
    "    Reshape((SPLIT_SIZE, SPLIT_SIZE, OUTPUT_DIM))\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2):\n",
    "    boxes1_t = tf.stack([boxes1[..., 0] - boxes1[..., 2] / 2.0,\n",
    "                         boxes1[..., 1] - boxes1[..., 3] / 2.0,\n",
    "                         boxes1[..., 0] + boxes1[..., 2] / 2.0,\n",
    "                         boxes1[..., 1] + boxes1[..., 3] / 2.0],\n",
    "                         axis = -1)\n",
    "                         \n",
    "    boxes2_t = tf.stack([boxes2[..., 0] - boxes2[..., 2] / 2.0,\n",
    "                         boxes2[..., 1] - boxes2[..., 3] / 2.0,\n",
    "                         boxes2[..., 0] + boxes2[..., 2] / 2.0,\n",
    "                         boxes2[..., 1] + boxes2[..., 3] / 2.0],\n",
    "                         axis = -1)\n",
    "\n",
    "    lu = tf.maximum(boxes1_t[..., :2], boxes2_t[..., :2])\n",
    "    rd = tf.minimum(boxes1_t[..., 2:], boxes2_t[..., 2:])\n",
    "\n",
    "    intersection = tf.maximum(0.0, rd - lu)\n",
    "    inter_square = intersection[..., 0] * intersection[..., 1]\n",
    "\n",
    "    square1 = boxes1[..., 2] * boxes1[..., 3]\n",
    "    square2 = boxes2[..., 2] * boxes2[..., 3]\n",
    "\n",
    "    union_square = tf.maximum(square1 + square2 - inter_square, 1e-10)\n",
    "\n",
    "    return tf.clip_by_value(inter_square / union_square, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(x, y):\n",
    "    return tf.reduce_sum(tf.square(y - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_loss(y_true, y_pred):\n",
    "    target = y_true[..., 0]\n",
    "\n",
    "    y_pred_extract = tf.gather_nd(y_pred, tf.where(target[:] == 1))\n",
    "    y_target_extract = tf.gather_nd(y_true, tf.where(target[:] == 1))\n",
    "\n",
    "    rescaler = tf.where(target[:] == 1) * SPLIT_SIZE\n",
    "    upscaler_1 = tf.concat([rescaler[:, 1:], tf.zeros([len(rescaler), 2], dtype=tf.int64)], axis=-1)\n",
    "\n",
    "    target_upscaler_2 = tf.repeat([[float(SPLIT_SIZE), float(SPLIT_SIZE), 224., 224.]], repeats=[len(rescaler)], axis=0) * tf.cast(y_target_extract[..., 1:5], dtype=tf.float32)\n",
    "\n",
    "    pred_1_upscaler_2 = tf.repeat([[float(SPLIT_SIZE), float(SPLIT_SIZE), 224., 224.]], repeats=[len(rescaler)], axis=0) * tf.cast(y_pred_extract[..., 1:5], dtype=tf.float32)\n",
    "\n",
    "    pred_2_upscaler_2 = tf.repeat([[float(SPLIT_SIZE), float(SPLIT_SIZE), 224., 224.]], repeats=[len(rescaler)], axis=0) * tf.cast(y_pred_extract[..., 6:10], dtype=tf.float32)\n",
    "\n",
    "    target_orig = tf.cast(upscaler_1, dtype=tf.float32) + target_upscaler_2\n",
    "    pred_1_orig = tf.cast(upscaler_1, dtype=tf.float32) + pred_1_upscaler_2\n",
    "    pred_2_orig = tf.cast(upscaler_1, dtype=tf.float32) + pred_2_upscaler_2\n",
    "\n",
    "\n",
    "    mask = tf.cast(tf.math.greater(compute_iou(target_orig, pred_2_orig), compute_iou(target_orig, pred_1_orig)), dtype=tf.int32)\n",
    "\n",
    "    y_pred_joined = tf.transpose(tf.concat([tf.expand_dims(y_pred_extract[..., 0], axis=0), tf.expand_dims(y_pred_extract[..., 5], axis=0)], axis=0))\n",
    "\n",
    "    obj_pred = tf.gather_nd(y_pred_joined, tf.stack([tf.range(len(rescaler)), mask], axis=-1))\n",
    "\n",
    "    object_loss = difference(tf.cast(obj_pred, dtype=tf.float32), tf.cast(tf.ones([len(rescaler)]), dtype=tf.float32))\n",
    "\n",
    "    # Noobj\n",
    "\n",
    "    y_pred_extract = tf.gather_nd(y_pred[..., 0:B*5], tf.where(target[:] == 0))\n",
    "\n",
    "    y_target_extract = tf.zeros(len(y_pred_extract))\n",
    "\n",
    "    no_object_loss_1 = difference(tf.cast(y_pred_extract[..., 0], dtype=tf.float32), tf.cast(y_target_extract, dtype=tf.float32))\n",
    "    \n",
    "    no_object_loss_2 = difference(tf.cast(y_pred_extract[..., 5], dtype=tf.float32), tf.cast(y_target_extract, dtype=tf.float32))\n",
    "    \n",
    "    no_object_loss = no_object_loss_1 + no_object_loss_2\n",
    "\n",
    "    # Obj class loss\n",
    "\n",
    "    y_pred_extract = tf.gather_nd(y_pred[..., 10:], tf.where(target[:] == 1))\n",
    "    class_extract = tf.gather_nd(y_true[...,5:], tf.where(target[:] == 1))\n",
    "\n",
    "    class_loss = difference(tf.cast(y_pred_extract, dtype=tf.float32), tf.cast(class_extract, dtype=tf.float32))\n",
    "\n",
    "    # Obj bounding box loss\n",
    "\n",
    "    y_pred_extract = tf.gather_nd(y_pred[..., 0:B*5], tf.where(target[:] == 1))\n",
    "    centre_joined = tf.stack([y_pred_extract[..., 1:3], y_pred_extract[..., 6:8]], axis=1)\n",
    "    centre_pred = tf.gather_nd(centre_joined, tf.stack([tf.range(len(rescaler)), mask], axis=-1))\n",
    "    centre_target = tf.gather_nd(y_true[..., 1:3], tf.where(target[:] == 1))\n",
    "\n",
    "    centre_loss = difference(centre_pred, centre_target)\n",
    "\n",
    "    size_joined = tf.stack([y_pred_extract[..., 3:5], y_pred_extract[..., 8:10]], axis=1)\n",
    "\n",
    "    size_pred = tf.gather_nd(size_joined, tf.stack([tf.range(len(rescaler)), mask], axis=-1))\n",
    "    size_target = tf.gather_nd(y_true[..., 3:5], tf.where(target[:] == 1))\n",
    "\n",
    "    size_loss = difference(tf.math.sqrt(tf.math.abs(size_pred)), tf.math.sqrt(tf.math.abs(size_target)))\n",
    "    box_loss = centre_loss + size_loss\n",
    "\n",
    "    lambda_coord = 5.0\n",
    "    lambda_no_obj = 0.5\n",
    "\n",
    "    loss = object_loss + (lambda_no_obj * no_object_loss) + tf.cast(lambda_coord * box_loss, dtype=tf.float32) + tf.cast(class_loss, dtype=tf.float32)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_true = generate_output([[0.210784, 0.616422, 0.127451, 0.232843, 2]])\n",
    "# y_true = generate_output([[0.509804, 0.411765, 0.107843, 0.245098, 3],\n",
    "#                           [0.210784, 0.616422, 0.127451, 0.232843, 2]])\n",
    "# y_true = np.expand_dims(y_true, axis=0)\n",
    "# y_pred = np.random.normal(size = (1, 7, 7, N_CLASSES+5*B))\n",
    "\n",
    "# y_pred[0][1][4] = [0.9,0.47,0.31,0.12,0.23, 1.0,0.2,0.6,0.1,0.95, 0.9,0.8,0.2,0.6,0.1,0.5,0.9,0.35]\n",
    "# y_pred[0][3][2] = [0.3,0.01,0.08,0.11,0.54, 0.98,0.56,0.88,0.1,0.24, 0.09,0.018,0.22,0.16,0.01,0.05,0.99,0.3]\n",
    "\n",
    "# yolo_loss(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_filepath = './yolo_resnet_50.h5'\n",
    "checkpoint_filepath = './yolo_efficientnet_b1.h5'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_filepath,\n",
    "    save_weights_only = True,\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min',\n",
    "    save_best_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 40:\n",
    "        return 1e-3\n",
    "    elif epoch >= 40 and epoch < 80:\n",
    "        return 5e-4\n",
    "    else:\n",
    "        return 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = yolo_loss,\n",
    "    optimizer = Adam(1e-3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_devices = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_visible_devices(devices=my_devices, device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2h35 d'entrainement sur RTX 2080 et 60 epochs\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 60,\n",
    "    callbacks = [lr_callback, callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO_PATH = './coco2017/val2017/' # A éviter si vous ne voulez pas que ça soit trop long\n",
    "COCO_PATH = './VOC2012/ValJPEGImages/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(filename):\n",
    "    try:\n",
    "        test_path = COCO_PATH + filename\n",
    "        print(test_path)\n",
    "\n",
    "        img = cv2.resize(cv2.imread(test_path), (H, W))\n",
    "\n",
    "        image = tf.io.decode_jpeg(tf.io.read_file(test_path))\n",
    "        image = tf.image.resize(image, [H, W])\n",
    "\n",
    "        output = model.predict(np.expand_dims(image, axis=0))\n",
    "\n",
    "        THRESH = .25\n",
    "\n",
    "        object_positions = tf.concat([tf.where(output[..., 0] >= THRESH), tf.where(output[..., 5] >= THRESH)], axis=0)\n",
    "        selected_output = tf.gather_nd(output, object_positions)\n",
    "        final_boxes = []\n",
    "        final_scores = []\n",
    "\n",
    "        for i, pos in enumerate(object_positions):\n",
    "            for j in range(2):\n",
    "                if selected_output[i][j*5] > THRESH:\n",
    "                    output_box = tf.cast(output[pos[0]][pos[1]][pos[2]][(j*5)+1:(j*5)+5], dtype=tf.float32)\n",
    "\n",
    "                    x_centre = (tf.cast(pos[1], dtype=tf.float32) + output_box[0]) * 32\n",
    "                    y_centre = (tf.cast(pos[2], dtype=tf.float32) + output_box[1]) * 32\n",
    "\n",
    "                    x_width, y_height = tf.math.abs(H * output_box[2]), tf.math.abs(W * output_box[3])\n",
    "                    x_min, y_min = int(x_centre - (x_width/2)), int(y_centre - (y_height/2))\n",
    "                    x_max, y_max = int(x_centre + (x_width/2)), int(y_centre + (y_height/2))\n",
    "\n",
    "                    if (x_min <= 0): x_min = 0\n",
    "                    if (y_min <= 0): y_min = 0\n",
    "                    if (x_max >= W): x_max = W\n",
    "                    if (y_max >= H): y_max = H\n",
    "\n",
    "                    final_boxes.append([x_min, y_min, x_max, y_max, str(classes[tf.argmax(selected_output[..., 10:], axis=-1)[i]])])\n",
    "                    final_scores.append(selected_output[i][j*5])\n",
    "\n",
    "        print(final_scores)\n",
    "        print('final_boxes', final_boxes)\n",
    "        final_boxes = np.array(final_boxes)\n",
    "\n",
    "        object_classes = final_boxes[..., 4]\n",
    "        nms_boxes = final_boxes[..., 0:4]\n",
    "\n",
    "        nms_output = tf.image.non_max_suppression(nms_boxes, final_scores, max_output_size=100, iou_threshold=0.2, score_threshold=float('-inf'))\n",
    "        print(nms_output)\n",
    "\n",
    "        for i in nms_output:\n",
    "            cv2.rectangle(\n",
    "          img,\n",
    "          (int(final_boxes[i][0]),int(final_boxes[i][1])),\n",
    "          (int(final_boxes[i][2]),int(final_boxes[i][3])),(0,0,255),1)\n",
    "            cv2.putText(\n",
    "          img,\n",
    "          final_boxes[i][-1],\n",
    "          (int(final_boxes[i][0]),int(final_boxes[i][1])+15),\n",
    "          cv2.FONT_HERSHEY_COMPLEX_SMALL,1,(2,225,155),1\n",
    "          )\n",
    "        print('--------------------------------')\n",
    "        \n",
    "        cv2.imwrite('./outputs/' + filename[:-4] + '_det' + '.jpg', cv2.resize(img, (256, 256)))\n",
    "\n",
    "    except:\n",
    "        print(\"No object found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(COCO_PATH):\n",
    "    model_test(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
