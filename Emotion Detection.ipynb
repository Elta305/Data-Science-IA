{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset disponible sur: https://www.kaggle.com/datasets/muhammadhananasghar/human-emotions-datasethes/code?select=Emotions+Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, MaxPool2D, Dense, InputLayer, Flatten, BatchNormalization, Input, Dropout, RandomFlip, RandomRotation, RandomContrast, Resizing, Rescaling, GlobalAveragePooling2D, Activation, MaxPooling2D, Add, Embedding, LayerNormalization, MultiHeadAttention, Permute\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy, FalsePositives, FalseNegatives, TruePositives, TrueNegatives, Precision, Recall, AUC, binary_accuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import L2, L1\n",
    "from tensorflow.train import BytesList, FloatList, Int64List, Example, Features, Feature\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"Emotion-Detection\", entity=\"fredlicombeau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"IM_SIZE\": 256,\n",
    "    \"LEARNING_RATE\": 1e-3,\n",
    "    \"N_EPOCHS\": 20,\n",
    "    \"DROPOUT_RATE\": 0.0,\n",
    "    \"REGULARIZATION_RATE\": 0.0,\n",
    "    \"N_FILTERS\": 6,\n",
    "    \"KERNEL_SIZE\": 3,\n",
    "    \"N_STRIDES\": 1,\n",
    "    \"POOL_SIZE\": 2,\n",
    "    \"N_DENSE_1\": 1024,\n",
    "    \"N_DENSE_2\": 128,\n",
    "    \"NUM_CLASSES\": 3,\n",
    "    \"PATCH_SIZE\": 16,\n",
    "    \"PROJ_DIM\": 768,\n",
    "    \"CLASS_NAMES\": [\"angry\", \"happy\", \"sad\"],\n",
    "}\n",
    "CONFIGURATION = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION = {\n",
    "#     \"BATCH_SIZE\": 32,\n",
    "#     \"IM_SIZE\": 256,\n",
    "#     \"LEARNING_RATE\": 1e-3,\n",
    "#     \"N_EPOCHS\": 20,\n",
    "#     \"DROPOUT_RATE\": 0.0,\n",
    "#     \"REGULARIZATION_RATE\": 0.0,\n",
    "#     \"N_FILTERS\": 6,\n",
    "#     \"KERNEL_SIZE\": 3,\n",
    "#     \"N_STRIDES\": 1,\n",
    "#     \"POOL_SIZE\": 2,\n",
    "#     \"N_DENSE_1\": 1024,\n",
    "#     \"N_DENSE_2\": 128,\n",
    "#     \"NUM_CLASSES\": 3,\n",
    "#     \"PATCH_SIZE\": 16,\n",
    "#     \"PROJ_DIM\": 768,\n",
    "#     \"CLASS_NAMES\": [\"angry\", \"happy\", \"sad\"],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = \"./dataset/Emotions Dataset/Emotions Dataset/train\"\n",
    "val_directory = \"./dataset/Emotions Dataset/Emotions Dataset/test\"\n",
    "CLASS_NAMES = [\"angry\", \"happy\", \"sad\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=CLASS_NAMES,\n",
    "    color_mode='rgb',\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=CLASS_NAMES,\n",
    "    color_mode='rgb',\n",
    "    batch_size=1,\n",
    "    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in val_dataset.take(1):\n",
    "#     print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(16):\n",
    "        ax = plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(images[i]/255.)\n",
    "        plt.title(CONFIGURATION[\"CLASS_NAMES\"][tf.argmax(labels[i], axis = 0).numpy()])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_layers = tf.keras.Sequential([\n",
    "    RandomRotation(factor=(-0.025, 0.025)),\n",
    "    RandomFlip(mode='horizontal'),\n",
    "    RandomContrast(factor=0.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_layer(image, label):\n",
    "    return augment_layers(image, training=True), label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutmix Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box(lamda):\n",
    "    r_x = tf.cast(tfp.distributions.Uniform(0, CONFIGURATION['IM_SIZE']).sample(1)[0], dtype=tf.int32)\n",
    "    r_y = tf.cast(tfp.distributions.Uniform(0, CONFIGURATION['IM_SIZE']).sample(1)[0], dtype=tf.int32)\n",
    "\n",
    "    r_w = tf.cast(CONFIGURATION['IM_SIZE']*tf.math.sqrt(1-lamda), dtype=tf.int32)\n",
    "    r_h = tf.cast(CONFIGURATION['IM_SIZE']*tf.math.sqrt(1-lamda), dtype=tf.int32)\n",
    "    \n",
    "    r_x = tf.clip_by_value(r_x - r_w//2, 0, CONFIGURATION['IM_SIZE'])\n",
    "    r_y = tf.clip_by_value(r_y - r_h//2, 0, CONFIGURATION['IM_SIZE'])\n",
    "\n",
    "    x_b_r = tf.clip_by_value(r_x + r_w//2, 0, CONFIGURATION['IM_SIZE'])\n",
    "    y_b_r = tf.clip_by_value(r_y + r_h//2, 0, CONFIGURATION['IM_SIZE'])\n",
    "\n",
    "    r_w = x_b_r - r_x\n",
    "    if r_w == 0:\n",
    "        r_w = 1\n",
    "\n",
    "    r_h = y_b_r - r_y\n",
    "    if r_h == 0:\n",
    "        r_h = 1\n",
    "    \n",
    "    return r_y, r_x, r_h, r_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix(train_dataset_1, train_dataset_2):\n",
    "    (image_1, label_1), (image_2, label_2) = train_dataset_1, train_dataset_2\n",
    "\n",
    "    lamda = tfp.distributions.Beta(0.2, 0.2)\n",
    "    lamda = lamda.sample(1)[0]\n",
    "    \n",
    "    r_y, r_x, r_h, r_w = box(lamda)\n",
    "\n",
    "    crop_2 = tf.image.crop_to_bounding_box(image_2, r_y, r_x, r_h, r_w)\n",
    "    pad_2 = tf.image.pad_to_bounding_box(crop_2, r_y, r_x, CONFIGURATION['IM_SIZE'], CONFIGURATION['IM_SIZE'])\n",
    "\n",
    "    crop_1 = tf.image.crop_to_bounding_box(image_1, r_y, r_x, r_h, r_w)\n",
    "    pad_1 = tf.image.pad_to_bounding_box(crop_1, r_y, r_x, CONFIGURATION['IM_SIZE'], CONFIGURATION['IM_SIZE'])\n",
    "\n",
    "    image = image_1 - pad_1 + pad_2\n",
    "\n",
    "    lamda = tf.cast(1 - (r_w*r_h)/(CONFIGURATION['IM_SIZE']*CONFIGURATION['IM_SIZE']), dtype=tf.float32)\n",
    "    label = lamda * tf.cast(label_1, dtype=tf.float32) + (1-lamda) * tf.cast(label_2, dtype=tf.float32)\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_1 = train_dataset.map(augment_layer, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset_2 = train_dataset.map(augment_layer, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "mixed_dataset = tf.data.Dataset.zip((train_dataset_1, train_dataset_2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = (\n",
    "    mixed_dataset\n",
    "    .map(cutmix, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = (\n",
    "    val_dataset\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_rescale_layers = tf.keras.Sequential([\n",
    "    Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
    "    Rescaling(1./255),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = (\n",
    "    training_dataset\n",
    "    .unbatch()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = (\n",
    "    validation_dataset\n",
    "    .unbatch()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(image, label):\n",
    "    bytes_feature = Feature(\n",
    "        bytes_list=BytesList(value=[image])\n",
    "    )\n",
    "    int_feature = Feature(\n",
    "        int64_list=Int64List(value=[label])\n",
    "    )\n",
    "\n",
    "    example = Example(\n",
    "        features=Features(feature={\n",
    "            'images': bytes_feature,\n",
    "            'labels': int_feature,\n",
    "        })\n",
    "    )\n",
    "\n",
    "    return example.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SHARDS = 10\n",
    "PATH = 'tfrecords/shard_{:02d}.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.uint8)\n",
    "    image = tf.io.encode_jpeg(image)\n",
    "    return image, tf.argmax(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = (\n",
    "    training_dataset\n",
    "    .map(encode_image)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for shard_number in range(NUM_SHARDS):\n",
    "    sharded_dataset = (\n",
    "        encoded_dataset\n",
    "        .shard(NUM_SHARDS, shard_number)\n",
    "        .as_numpy_iterator()\n",
    "    )\n",
    "\n",
    "    with tf.io.TFRecordWriter(PATH.format(shard_number)) as file_writer:\n",
    "        for encoded_image, encoded_label in sharded_dataset:\n",
    "            example = create_example(encoded_image, encoded_label)\n",
    "            file_writer.write(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_dataset = tf.data.TFRecordDataset(filenames=[PATH.format(p) for p in range(NUM_SHARDS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecords(example):\n",
    "    feature_description = {\n",
    "        'images': tf.io.FixedLenFeature([], tf.string),\n",
    "        'labels': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    \n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example[\"images\"] = tf.io.decode_jpeg(example[\"images\"], channels=3)\n",
    "\n",
    "    return example[\"images\"], example[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dataset = (\n",
    "    recons_dataset\n",
    "    .map(parse_tfrecords)\n",
    "    .batch(CONFIGURATION[\"BATCH_SIZE\"])\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model = tf.keras.Sequential([\n",
    "    InputLayer(input_shape = (None, None, 3),),\n",
    "\n",
    "    resize_rescale_layers,\n",
    "\n",
    "    Conv2D(filters=CONFIGURATION[\"N_FILTERS\"], kernel_size=CONFIGURATION[\"KERNEL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"], padding='valid', activation=\"relu\", kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=CONFIGURATION[\"POOL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"]*2),\n",
    "    Dropout(rate=CONFIGURATION[\"DROPOUT_RATE\"]),\n",
    "\n",
    "    Conv2D(filters=CONFIGURATION[\"N_FILTERS\"]*2 + 4, kernel_size=CONFIGURATION[\"KERNEL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"], padding='valid', activation=\"relu\", kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=CONFIGURATION[\"POOL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"]*2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(CONFIGURATION[\"N_DENSE_1\"], activation=\"relu\", kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
    "    BatchNormalization(),\n",
    "    Dropout(rate=CONFIGURATION[\"DROPOUT_RATE\"]),\n",
    "\n",
    "    Dense(CONFIGURATION[\"N_DENSE_2\"], activation=\"relu\", kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(CONFIGURATION[\"NUM_CLASSES\"], activation=\"softmax\")\n",
    "])\n",
    "\n",
    "lenet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv2D(Layer):\n",
    "    def __init__(self, n_filters, kernel_size, n_strides, padding='valid'):\n",
    "        super(CustomConv2D, self).__init__(name='custom_conv2d')\n",
    "\n",
    "        self.conv = Conv2D(\n",
    "            filters = n_filters,\n",
    "            kernel_size = kernel_size,\n",
    "            activation = 'relu',\n",
    "            strides = n_strides,\n",
    "            padding = padding\n",
    "        )\n",
    "\n",
    "        self.batch_norm = BatchNormalization()\n",
    "    \n",
    "    def call(self, x, training=True):\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(Layer):\n",
    "    def __init__(self, n_channels, n_strides=1):\n",
    "        super(ResidualBlock, self).__init__(name='res_block')\n",
    "\n",
    "        self.dotted = (n_strides != 1)\n",
    "\n",
    "        self.custom_conv_1 = CustomConv2D(n_channels, 3, n_strides, padding='same')\n",
    "        self.custom_conv_2 = CustomConv2D(n_channels, 3, 1, padding='same')\n",
    "\n",
    "        self.activation = Activation('relu')\n",
    "\n",
    "        if self.dotted:\n",
    "            self.custom_conv_3 = CustomConv2D(n_channels, 1, n_strides)\n",
    "        \n",
    "    def call(self, input, training):\n",
    "        x = self.custom_conv_1(input, training)\n",
    "        x = self.custom_conv_2(x, training)\n",
    "\n",
    "        if self.dotted:\n",
    "            x_add = self.custom_conv_3(input, training)\n",
    "            x_add = Add()([x, x_add])\n",
    "        else:\n",
    "            x_add = Add()([x, input])\n",
    "\n",
    "        return self.activation(x_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34(Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet34, self).__init__(name='resnet_34')\n",
    "\n",
    "        self.conv_1 = CustomConv2D(64, 7, 2, padding='same')\n",
    "        self.max_pool = MaxPooling2D(3, 2)\n",
    "\n",
    "        self.conv_2_1 = ResidualBlock(64)\n",
    "        self.conv_2_2 = ResidualBlock(64)\n",
    "        self.conv_2_3 = ResidualBlock(64)\n",
    "\n",
    "        self.conv_3_1 = ResidualBlock(128, 2)\n",
    "        self.conv_3_2 = ResidualBlock(128)\n",
    "        self.conv_3_3 = ResidualBlock(128)\n",
    "        self.conv_3_4 = ResidualBlock(128)\n",
    "\n",
    "        self.conv_4_1 = ResidualBlock(256, 2)\n",
    "        self.conv_4_2 = ResidualBlock(256)\n",
    "        self.conv_4_3 = ResidualBlock(256)\n",
    "        self.conv_4_4 = ResidualBlock(256)\n",
    "        self.conv_4_5 = ResidualBlock(256)\n",
    "        self.conv_4_6 = ResidualBlock(256)\n",
    "\n",
    "        self.conv_5_1 = ResidualBlock(512, 2)\n",
    "        self.conv_5_2 = ResidualBlock(512)\n",
    "        self.conv_5_3 = ResidualBlock(512)\n",
    "\n",
    "        self.global_pool = GlobalAveragePooling2D()\n",
    "\n",
    "        self.fc_3 = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation='softmax')\n",
    "    \n",
    "    def call(self, x, training=True):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        x = self.conv_2_1(x, training)\n",
    "        x = self.conv_2_2(x, training)\n",
    "        x = self.conv_2_3(x, training)\n",
    "\n",
    "        x = self.conv_3_1(x, training)\n",
    "        x = self.conv_3_2(x, training)\n",
    "        x = self.conv_3_3(x, training)\n",
    "        x = self.conv_3_4(x, training)\n",
    "\n",
    "        x = self.conv_4_1(x, training)\n",
    "        x = self.conv_4_2(x, training)\n",
    "        x = self.conv_4_3(x, training)\n",
    "        x = self.conv_4_4(x, training)\n",
    "        x = self.conv_4_5(x, training)\n",
    "        x = self.conv_4_6(x, training)\n",
    "\n",
    "        x = self.conv_5_1(x, training)\n",
    "        x = self.conv_5_2(x, training)\n",
    "        x = self.conv_5_3(x, training)\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "\n",
    "        return self.fc_3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_34 = ResNet34()\n",
    "resnet_34(tf.zeros([1, 256, 256, 3]), training=False)\n",
    "resnet_34.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning with EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = tf.keras.applications.efficientnet.EfficientNetB4(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.Sequential([\n",
    "    Input(shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3)),\n",
    "    backbone,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(CONFIGURATION[\"N_DENSE_1\"], activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dense(CONFIGURATION[\"N_DENSE_2\"], activation=\"relu\"),\n",
    "    Dense(CONFIGURATION[\"NUM_CLASSES\"], activation=\"softmax\"),\n",
    "])\n",
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FineTuning EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.trainable = False # Lancer une fois en False puis en True sans lancer les 2 cases suivantes. Relancer le compile avec le learning rate /100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3)),\n",
    "\n",
    "x = backbone(input, training=False)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(CONFIGURATION[\"N_DENSE_1\"], activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(CONFIGURATION[\"N_DENSE_2\"], activation=\"relu\")(x)\n",
    "output = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation=\"softmax\")(x)\n",
    "\n",
    "finetuned_model = Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patch Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread(\"./dataset/Emotions Dataset/Emotions Dataset/test/sad/123731.jpg\")\n",
    "test_image = cv2.resize(test_image, (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = tf.image.extract_patches(images=tf.expand_dims(test_image, axis=0),\n",
    "                                   sizes=[1, CONFIGURATION[\"PATCH_SIZE\"], CONFIGURATION[\"PATCH_SIZE\"], 1],\n",
    "                                   strides=[1, CONFIGURATION[\"PATCH_SIZE\"], CONFIGURATION[\"PATCH_SIZE\"], 1],\n",
    "                                   rates=[1, 1, 1, 1],\n",
    "                                   padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(patches.shape)\n",
    "patches = tf.reshape(patches, (patches.shape[0], -1, 768))\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "for i in range(patches.shape[1]):\n",
    "    ax = plt.subplot(16, 16, i+1)\n",
    "    plt.imshow(tf.reshape(patches[0, i, :], (16, 16, 3)))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(Layer):\n",
    "    def __init__(self, N_PATCHES, HIDDEN_SIZE):\n",
    "        super(PatchEncoder, self).__init__(name='patch_encoder')\n",
    "\n",
    "        self.linear_projection = Dense(HIDDEN_SIZE)\n",
    "        self.positional_embedding = Embedding(N_PATCHES, HIDDEN_SIZE)\n",
    "        self.N_PATCHES = N_PATCHES\n",
    "    \n",
    "    def call(self, x):\n",
    "        patches = tf.image.extract_patches(images=x,\n",
    "                                   sizes=[1, CONFIGURATION[\"PATCH_SIZE\"], CONFIGURATION[\"PATCH_SIZE\"], 1],\n",
    "                                   strides=[1, CONFIGURATION[\"PATCH_SIZE\"], CONFIGURATION[\"PATCH_SIZE\"], 1],\n",
    "                                   rates=[1, 1, 1, 1],\n",
    "                                   padding='VALID')\n",
    "        \n",
    "        patches = tf.reshape(patches, (tf.shape(patches)[0], -1, patches.shape[-1]))\n",
    "        embedding_input = tf.range(start=0, limit=self.N_PATCHES, delta=1)\n",
    "        output = self.linear_projection(patches) + self.positional_embedding(embedding_input)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_enc = PatchEncoder(256, 768)\n",
    "patch_enc(tf.zeros([32, 256, 256, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, N_HEADS, HIDDEN_SIZE):\n",
    "        super(TransformerEncoder, self).__init__(name='transformer_encoder')\n",
    "\n",
    "        self.layer_norm_1 = LayerNormalization()\n",
    "        self.layer_norm_2 = LayerNormalization()\n",
    "        \n",
    "        self.multi_head_att = MultiHeadAttention(N_HEADS, HIDDEN_SIZE)\n",
    "\n",
    "        self.dense_1 = Dense(HIDDEN_SIZE, activation=tf.nn.gelu)\n",
    "        self.dense_2 = Dense(HIDDEN_SIZE, activation=tf.nn.gelu)\n",
    "    \n",
    "    def call(self, input):\n",
    "        x_1 = self.layer_norm_1(input)\n",
    "        x_1 = self.multi_head_att(x_1, x_1)\n",
    "\n",
    "        x_1 = Add()([x_1, input])\n",
    "\n",
    "        x_2 = self.layer_norm_2(x_1)\n",
    "        x_2 = self.dense_1(x_2)\n",
    "        output = self.dense_2(x_2)\n",
    "        output = Add()([output, x_1])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_enc = TransformerEncoder(8, 768)\n",
    "trans_enc(tf.zeros([1, 256, 768]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ViT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(Model):\n",
    "    def __init__(self, N_HEADS, HIDDEN_SIZE, N_PATCHES, N_LAYERS, N_DENSE_UNITS):\n",
    "        super(ViT, self).__init__(name='vision_transformer')\n",
    "\n",
    "        self.N_LAYERS = N_LAYERS\n",
    "        self.patch_encoder = PatchEncoder(N_PATCHES, HIDDEN_SIZE)\n",
    "        self.trans_encoders = [TransformerEncoder(N_HEADS, HIDDEN_SIZE) for _ in range(N_LAYERS)]\n",
    "        self.dense_1 = Dense(N_DENSE_UNITS, tf.nn.gelu)\n",
    "        self.dense_2 = Dense(N_DENSE_UNITS, tf.nn.gelu)\n",
    "        self.dense_3 = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation='softmax')\n",
    "    \n",
    "    def call(self, input, training=True):\n",
    "        x = self.patch_encoder(input)\n",
    "\n",
    "        for i in range(self.N_LAYERS):\n",
    "            x = self.trans_encoders[i](x)\n",
    "        \n",
    "        x = Flatten()(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        \n",
    "        return self.dense_3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(4, 768, 256, 2, 128)\n",
    "vit(tf.zeros([32, 256, 256, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_rescale_hf = tf.keras.Sequential([\n",
    "       Resizing(224, 224),\n",
    "       Rescaling(1./255),\n",
    "       Permute((3,1,2))              \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, TFViTModel, ViTConfig, ViTModel\n",
    "\n",
    "base_model = TFViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "inputs = Input(shape = (256,256,3))\n",
    "x = resize_rescale_hf(inputs)\n",
    "x = base_model.vit(x)[0][:,0,:]\n",
    "output = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = 'softmax')(x)\n",
    "\n",
    "hf_model = tf.keras.Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread(\"./dataset/Emotions Dataset/Emotions Dataset/train/happy/387249.jpg\")\n",
    "test_image = cv2.resize(test_image, (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model(tf.expand_dims(test_image, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Attention Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, TFViTModel, ViTConfig\n",
    "\n",
    "configuration = ViTConfig()\n",
    "configuration.output_attentions = True\n",
    "\n",
    "base_model = TFViTModel.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"google/vit-base-patch16-224-in21k\",\n",
    "    config = configuration,\n",
    ")\n",
    "inputs = Input(shape = (256,256,3))\n",
    "x = resize_rescale_hf(inputs)\n",
    "x = base_model.vit(x)['attentions']\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    'best_weights', \n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogConfMatrix(Callback):\n",
    "  def on_epoch_end(self, epoch, logs):\n",
    "    predicted = []\n",
    "    labels = []\n",
    "\n",
    "    for im, label in validation_dataset:\n",
    "      predicted.append(hf_model(im))\n",
    "      labels.append(label.numpy())\n",
    "\n",
    "    pred = np.concatenate([np.argmax(predicted[:-1], axis = -1).flatten(), np.argmax(predicted[-1], axis = -1).flatten()])\n",
    "    lab = np.concatenate([np.argmax(labels[:-1], axis = -1).flatten(), np.argmax(labels[-1], axis = -1).flatten()])\n",
    "    \n",
    "    cm = wandb.plot.confusion_matrix(\n",
    "        y_true=lab,\n",
    "        preds=pred,\n",
    "        class_names=CONFIGURATION[\"CLASS_NAMES\"])\n",
    "        \n",
    "    wandb.log({\"conf_mat\": cm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogResultsTable(Callback):\n",
    "  def on_epoch_end(self, epoch, logs):\n",
    "    \n",
    "    columns=[\"image\", \"Predicted\", \"Label\"]\n",
    "    \n",
    "    val_table = wandb.Table(columns = columns)\n",
    "\n",
    "    \n",
    "    for im, label in validation_dataset.take(25):\n",
    "\n",
    "      pred = CONFIGURATION[\"CLASS_NAMES\"][tf.argmax(hf_model(im), axis = -1).numpy()[0]]\n",
    "      label = CONFIGURATION[\"CLASS_NAMES\"][tf.argmax(label, axis = -1).numpy()[0]]\n",
    "\n",
    "      row = [wandb.Image(im), pred, label]\n",
    "      \n",
    "      val_table.add_data(*row)\n",
    "\n",
    "      \n",
    "    wandb.log({\"Model Results\" : val_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = CategoricalCrossentropy()\n",
    "# loss_function = SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), TopKCategoricalAccuracy(k=2, name=\"top_k_accuracy\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quant_aware_model.compile(optimizer=Adam(learning_rate=CONFIGURATION['LEARNING_RATE']),\n",
    "#                     loss=loss_function,\n",
    "#                     metrics=metrics)\n",
    "# hf_model.compile(optimizer=Adam(learning_rate=5e-5), # 5e-5\n",
    "#                     loss=loss_function,\n",
    "#                     metrics=metrics)\n",
    "vit.compile(optimizer=Adam(learning_rate=CONFIGURATION['LEARNING_RATE']),\n",
    "                    loss=loss_function,\n",
    "                    metrics=metrics)\n",
    "# finetuned_model.compile(optimizer=Adam(learning_rate=CONFIGURATION['LEARNING_RATE']), # /100\n",
    "#                     loss=loss_function,\n",
    "#                     metrics=metrics)\n",
    "# pretrained_model.compile(optimizer=Adam(learning_rate=CONFIGURATION['LEARNING_RATE']),\n",
    "#                     loss=loss_function,\n",
    "#                     metrics=metrics)\n",
    "# resnet_34.compile(optimizer=Adam(learning_rate=CONFIGURATION['LEARNING_RATE']*10),\n",
    "#                     loss=loss_function,\n",
    "#                     metrics=metrics)\n",
    "# lenet_model.compile(optimizer=Adam(learning_rate=CONFIGURATION['LEARNING_RATE']),\n",
    "#                     loss=loss_function,\n",
    "#                     metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_devices = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_visible_devices(devices= my_devices, device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = quant_aware_model.fit(training_dataset.take(20),\n",
    "#                           validation_data=validation_dataset, \n",
    "#                           epochs=1,\n",
    "#                           verbose=1)\n",
    "# history = hf_model.fit(training_dataset.take(20),\n",
    "#                        validation_data = validation_dataset,\n",
    "#                        epochs = 3,\n",
    "#                        verbose = 1,\n",
    "#                        callbacks = [WandbCallback(), LogConfMatrix(), LogResultsTable()]) # Trop coûteux en mémoire de GPU -> 9Go. A tester sur Google Collab.\n",
    "history = vit.fit(training_dataset.take(20),\n",
    "                          validation_data=validation_dataset, \n",
    "                          epochs=1,\n",
    "                          verbose=1)\n",
    "# history = finetuned_model.fit(training_dataset.take(10),\n",
    "#                           validation_data=validation_dataset, \n",
    "#                           epochs=CONFIGURATION[\"N_EPOCHS\"],\n",
    "#                           verbose=1)\n",
    "# history = pretrained_model.fit(training_dataset.take(10),\n",
    "#                           validation_data=validation_dataset, \n",
    "#                           epochs=CONFIGURATION[\"N_EPOCHS\"],\n",
    "#                           verbose=1)\n",
    "# history = resnet_34.fit(training_dataset,\n",
    "#                           validation_data=validation_dataset, \n",
    "#                           epochs=CONFIGURATION[\"N_EPOCHS\"]*3,\n",
    "#                           verbose=1,\n",
    "#                           callbacks=[checkpoint_callback])\n",
    "# history = lenet_model.fit(training_dataset,\n",
    "#                           validation_data=validation_dataset,\n",
    "#                           epochs=CONFIGURATION[\"N_EPOCHS\"],\n",
    "#                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'val_accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.load_weights('best_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.evaluate(validation_dataset)\n",
    "# resnet_34.evaluate(validation_dataset)\n",
    "# lenet_model.evaluate(validation_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread(\"./dataset/Emotions Dataset/Emotions Dataset/test/happy/520857.jpg\")\n",
    "test_image = cv2.resize(test_image, (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]))\n",
    "\n",
    "im = tf.constant(test_image, dtype=tf.float32)\n",
    "\n",
    "im = tf.expand_dims(im, axis=0)\n",
    "\n",
    "print(pretrained_model(im))\n",
    "print(CLASS_NAMES[tf.argmax(pretrained_model(im), axis=-1).numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread(\"./dataset/Emotions Dataset/Emotions Dataset/test/sad/105639.jpg\")\n",
    "\n",
    "im = tf.constant(test_image, dtype=tf.float32)\n",
    "\n",
    "im = tf.expand_dims(im, axis=0)\n",
    "\n",
    "print(resnet_34(im))\n",
    "print(CLASS_NAMES[tf.argmax(resnet_34(im), axis=-1).numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread(\"./dataset/Lilian.png\")\n",
    "\n",
    "im = tf.constant(test_image, dtype=tf.float32)\n",
    "\n",
    "im = tf.expand_dims(im, axis=0)\n",
    "\n",
    "print(resnet_34(im))\n",
    "print(CLASS_NAMES[tf.argmax(resnet_34(im), axis=-1).numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(16):\n",
    "        ax = plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(images[i]/255.)\n",
    "        plt.title(\"True Label: \" + CONFIGURATION[\"CLASS_NAMES\"][tf.argmax(labels[i], axis = -1).numpy()]  + \"\\nPredicted Label: \" \n",
    "              + CONFIGURATION[\"CLASS_NAMES\"][int(tf.argmax(pretrained_model(tf.expand_dims(images[i], axis = 0)), axis =-1).numpy()[0])])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Map Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_backbone = tf.keras.applications.vgg16.VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_backbone.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_conv(layer_name):\n",
    "    if 'conv' in layer_name:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = [layer.output for layer in vgg_backbone.layers[1:] if is_conv(layer.name)]\n",
    "feature_map_model = Model(\n",
    "    inputs=vgg_backbone.input,\n",
    "    outputs=feature_maps\n",
    ")\n",
    "\n",
    "feature_map_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread(\"./dataset/Emotions Dataset/Emotions Dataset/test/happy/520857.jpg\")\n",
    "test_image = cv2.resize(test_image, (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]))\n",
    "\n",
    "im = tf.constant(test_image, dtype=tf.float32)\n",
    "im = tf.expand_dims(im, axis=0)\n",
    "\n",
    "f_maps = feature_map_model.predict(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(f_maps)):\n",
    "    print(f_maps[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(f_maps)):\n",
    "    plt.figure(figsize=(256, 256))\n",
    "    f_size = f_maps[i].shape[1]\n",
    "    n_channels = f_maps[i].shape[-1]\n",
    "    joint_maps = np.ones((f_size, f_size*n_channels))\n",
    "    \n",
    "    axs = plt.subplot(len(f_maps), 1, i+1)\n",
    "\n",
    "    for j in range(n_channels):\n",
    "        joint_maps[:, f_size*j:f_size*(j+1)] = f_maps[i][..., j]\n",
    "\n",
    "    plt.imshow(joint_maps[:, 0:512])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = tf.keras.applications.efficientnet.EfficientNetB5(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3),\n",
    ")\n",
    "backbone.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = backbone.output\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense( CONFIGURATION[\"N_DENSE_1\"], activation = \"relu\")(x)\n",
    "x = Dense( CONFIGURATION[\"N_DENSE_2\"], activation = \"relu\")(x)\n",
    "output = Dense( CONFIGURATION[\"NUM_CLASSES\"], activation = \"softmax\")(x)\n",
    "\n",
    "pretrained_model = Model(backbone.inputs, output)\n",
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"./dataset/Emotions Dataset/Emotions Dataset/train/happy/202291.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread(img_path)\n",
    "test_image = cv2.resize(test_image, (CONFIGURATION[\"IM_SIZE\"] ,CONFIGURATION[\"IM_SIZE\"]))\n",
    "im = tf.constant(test_image, dtype = tf.float32)\n",
    "img_array = tf.expand_dims(im, axis = 0)\n",
    "print(img_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pretrained_model.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_name = \"top_activation\"\n",
    "last_conv_layer = pretrained_model.get_layer(last_conv_layer_name)\n",
    "last_conv_layer_model = Model(pretrained_model.inputs, last_conv_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_layer_names = [\n",
    "    \"global_average_pooling2d\",\n",
    "    \"dense\",\n",
    "    \"dense_1\",\n",
    "    \"dense_2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_input = Input(shape=(8, 8, 2048))\n",
    "x = classifier_input \n",
    "for layer_name in classifier_layer_names:\n",
    "    x = pretrained_model.get_layer(layer_name)(x)\n",
    "classifier_model = Model(classifier_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    last_conv_layer_output = last_conv_layer_model(img_array)\n",
    "    preds = classifier_model(last_conv_layer_output)\n",
    "    top_pred_index = tf.argmax(preds[0])\n",
    "    top_class_channel = preds[:, top_pred_index]\n",
    "\n",
    "grads = tape.gradient(top_class_channel, last_conv_layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
    "for i in range(pooled_grads.shape[0]):\n",
    "    last_conv_layer_output[:, :, i] *= pooled_grads[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = np.sum(last_conv_layer_output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = tf.nn.relu(heatmap)\n",
    "plt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_heatmap = cv2.resize(np.array(heatmap), (256, 256))\n",
    "plt.matshow(resized_heatmap*2555+img_array[0,:,:,0]/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to Onnx format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.save('vit_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.save('vit_finetuned.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Tensorflow SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tf2onnx.convert --saved-model vit_finetuned/ --output vit_onnx.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "import onnxruntime as rt\n",
    "\n",
    "spec = (tf.TensorSpec(\n",
    "    (None, CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3),\n",
    "    tf.float32, name=\"input\"),)\n",
    "\n",
    "output_path = \"vit_keras.onnx\"\n",
    "\n",
    "model_proto, _ = tf2onnx.convert.from_keras(hf_model, input_signature=spec, opset=13, output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread(\"/dataset/Emotions Dataset/Emotions Dataset/test/sad/123731.jpg\")\n",
    "test_image = cv2.resize(test_image, (CONFIGURATION[\"IM_SIZE\"] ,CONFIGURATION[\"IM_SIZE\"]))\n",
    "im = test_image.astype(np.float32)\n",
    "\n",
    "im = np.expand_dims(im, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PREDICTIONS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "providers=['CPUExecutionProvider']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = rt.InferenceSession(\"./vit_keras.onnx\", providers=providers)\n",
    "m = rt.InferenceSession(\"./vit_quantized.onnx\", providers=providers)\n",
    "t1 = time.time()\n",
    "\n",
    "for _ in range(N_PREDICTIONS):\n",
    "  onnx_pred = m.run(['dense'], {\"input\": im})\n",
    "print(\"Time for a single Prediction\", (time.time() - t1)/N_PREDICTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(onnx_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "for _ in range(N_PREDICTIONS):\n",
    "  hf_model(im)\n",
    "print(\"Time for a single Prediction\", (time.time() - t1)/N_PREDICTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf, gpu = 0.15s\n",
    "# tf, cpu = 0.8s\n",
    "# tf_size = 1000MB\n",
    "\n",
    "# onnx, cpu = 0.5s\n",
    "# onnx, gpu = 0.025s\n",
    "# onnx_size = 328MB\n",
    "\n",
    "# onnx_quantized, cpu = 0.4s\n",
    "# onnx_quantized, gpu = 0.3s\n",
    "# onnx_quantized_size = 83MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization with Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fp32 = 'vit_keras.onnx'\n",
    "# model_quant = 'vit_quantized.onnx'\n",
    "model_fp32 = 'eff_keras.onnx'\n",
    "model_quant = 'eff_quantized.onnx'\n",
    "\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type = QuantType.QUInt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model):\n",
    "  total, acc = 0,0\n",
    "  for im, label in validation_dataset:\n",
    "    onnx_pred = model.run(output_names, {\"input\": np.array(im)})\n",
    "\n",
    "    if(int(np.argmax(onnx_pred, axis = -1)[0][0]) == int(np.argmax(label, axis = -1)[0])):\n",
    "      acc += 1\n",
    "\n",
    "    total += 1\n",
    "  return acc/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = rt.InferenceSession(\"vit_keras.onnx\", providers=providers)\n",
    "m_q = rt.InferenceSession(\"vit_quantized.onnx\", providers=providers)\n",
    "print(accuracy(m_q))\n",
    "print(accuracy(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/dataset/Emotions Dataset/Emotions Dataset/test/happy/553112.jpg\"\n",
    "test_image = cv2.imread(img_path)\n",
    "print(test_image.shape)\n",
    "test_image = cv2.resize(test_image, (CONFIGURATION[\"IM_SIZE\"] ,CONFIGURATION[\"IM_SIZE\"]))\n",
    "im = np.float32(test_image)\n",
    "img_array = np.expand_dims(im, axis = 0)\n",
    "print(img_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone,\n",
    "\n",
    "x = GlobalAveragePooling2D()(backbone.output)\n",
    "x = Dense(CONFIGURATION[\"N_DENSE_1\"], activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(CONFIGURATION[\"N_DENSE_2\"], activation=\"relu\")(x)\n",
    "output = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation=\"softmax\")(x)\n",
    "\n",
    "pretrained_func_model = tf.keras.Model(inputs=backbone.input, outputs=output)\n",
    "\n",
    "pretrained_func_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quant_aware = tfmot.quantization.keras.quantize_model(lenet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quantization_to_conv(layer):\n",
    "    if \"conv\" in layer.name:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_aware_eff = tf.keras.models.clone_model(\n",
    "    pretrained_func_model, clone_function=apply_quantization_to_conv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_aware_eff.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(quant_aware_eff)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.load_weights(\"eff_keras.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_data_gen():\n",
    "    for input_value, j in training_dataset.take(20):\n",
    "        yield [input_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(pretrained_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "converter.representative_dataset = representative_data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\"/quantized_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tflite_model_file = tflite_models_dir/\"eff_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.save(\"eff_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFLite Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflite_runtime as tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread(\"/dataset/Emotions Dataset/Emotions Dataset/train/happy/148266.jpg\")\n",
    "test_image = cv2.resize(test_image, (CONFIGURATION[\"IM_SIZE\"] ,CONFIGURATION[\"IM_SIZE\"]))\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "#print(CONFIGURATION['CLASS_NAMES'][tf.argmax(pretrained_model(im), axis = -1).numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tflite.Interpreter(model_path=\"eff_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "# test_image = im.numpy().astype(input_details[\"dtype\"])\n",
    "\n",
    "interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "interpreter.invoke()\n",
    "\n",
    "output = interpreter.get_tensor(output_details[\"index\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CONFIGURATION['CLASS_NAMES'][np.argmax(output)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy of Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model_path):\n",
    "    total, correct = 0,0\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    for im, label in validation_dataset:\n",
    "        \n",
    "        test_image = im.numpy().astype(input_details[\"dtype\"])\n",
    "\n",
    "        interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "        if(int(np.argmax(output)) == int(np.argmax(label, axis = -1)[0])):\n",
    "            correct += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(\"eff_model.tflite\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "labels = []\n",
    "\n",
    "for im, label in validation_dataset:\n",
    "    predicted.append(pretrained_model(im))\n",
    "    labels.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.concatenate([np.argmax(labels[:-1], axis = -1).flatten(), np.argmax(labels[-1], axis = -1).flatten()]))\n",
    "print(np.concatenate([np.argmax(predicted[:-1], axis = -1).flatten(), np.argmax(predicted[-1], axis = -1).flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.concatenate([np.argmax(labels[:-1], axis = -1).flatten(), np.argmax(labels[-1], axis = -1).flatten()])\n",
    "lab = np.concatenate([np.argmax(predicted[:-1], axis = -1).flatten(), np.argmax(predicted[-1], axis = -1).flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(lab, pred)\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
